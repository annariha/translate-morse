---
title: "Decode Morsecode with keras in R"
author: "Anna Elisabeth Riha"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
if(!requireNamespace("pacman"))install.packages("pacman")
pacman::p_load(tictoc, tidyverse, gridExtra, fs, here, data.table, ggplot2, reticulate, tensorflow, keras)
```

# Intro: LSTM Sequence-to-Sequence Model

As presented by Chris Manning and Richard Socher in a Lecture at Stanford University (School of Engineering) (available at https://www.youtube.com/watch?v=QuELiw8tbx8&feature=youtu.be&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&t=1190), the simplest encoder-decoder recurrent neural network model can be described via an encoder model with $h_t = \phi(h_{t-1}, x_t) = f(W^{(hh)}h_{t-1} + W^{(hx)}x_t)$ where $^{(hh)}$ denotes a hidden-to-hidden connection and a decoder model where $h_t = \phi(h_{t-1}) = f(W^{(hh)}h_{t-1})$ with a final softmax-Layer that generates outputs $y_t = softmax(W^{(S)}h_t)$.
The goal then is to minimize the Cross-Entropy Error, i.e. find $\max_{\theta}\frac{1}{N} \sum^N_{n=1} \log p_\theta(y^{(n)}| x^{(n)})$ for all target words $y^{(n)}$ conditioned on source/input words $x^{(n)}$.

In the Lecture, Richard Socher presents how this recurrent encoder-decoder setup can be extended via e.g. training different weights for encoding and decoding or using stacked recurrent layers as well as other extensions like bidirectional encoders. 

Instead of extending recurrent neural networks, the Lecture continues with introducing Long-Short Term Memory (LSTM) cells that allow to build a "better type of recurrent unit" that (among a wide range of other applications) is well suited for translating sequences.
LSTM networks were first introduced by Hochreiter & Schmidhuber in 1997. 
The authors developed LSTM networks as a method that "enforc[es] constant error flow through constant error carousels within special units" (see Hochreiter & Schmidhuber (1997)).
These units are so-called LSTM cells that consist of an input gate, forget gate and output gate. 
In this setup the gate units are connected multiplicatively and learn when to open and when to close (for detailed information see e.g. Hochreiter & Schmidhuber (1997)).
Following Richard Socher, the advantage of LSTMs when learning sequences is broadly speaking that LSTM models are able to "keep around" memories due to their architecture and thereby are able capture long-distance dependencies.

```{r}
# add picture of LSTM cell? 
```

# Translating Morsecode with LSTMs

The following presents a sequence-to-sequence LSTM model for decoding morsecode. 
To achieve this, an example for such an LSTM model for translating French to English is adjusted for decoding Morsecode to English alphabet, see the original example here  https://keras.rstudio.com/articles/examples/lstm_seq2seq.html .
As in the original example, the data is taken from http://www.manythings.org/anki/fra-eng.zip where sequence pairs of French and English short sentences are available.
As in the original example, input and target data are short sentences with the only difference that here the input sequences are translated to Morsecode first while the target sequences are English.

```{r}
# create directory
dir_create("Data")
# download data
temp <- tempfile()
download.file("http://www.manythings.org/anki/fra-eng.zip",temp)
data <- read.table(unz(temp, "Data/fra.txt"))
unlink(temp)
```

```{r}
# code partly taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

data_path = "./Data/fra.txt"
num_samples = 1000 # number of samples selected for training
text1 <- fread(data_path, sep="\t", header=FALSE, nrows=num_samples)
```

```{r}
# inspect sentence pairs
text1[1:2,1:2]
```

## Training 

Following the example from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html for training a LSTM model, an encoder LSTM network takes as input the sequences in Morsecode  and creates state vectors from these inputs. 
The last LSTM states are kept as input for the decoding model while the actual outputs of this encoder LSTM model are discarded.
Then, this last state vector of the encoder serves as the initial state for the decoder LSTM network.
In general, the strategy of taking the output of one model as input for another model is also referred to as "teacher forcing" in deep learning approaches.
Goodfellow et al. (2016) explain that teacher forcing is a technique when training a network that allows connections between outputs and hidden states. 
For training with teacher forcing, the known true output at time $t$ is used as input at time $t+1$ rather than the output generated by the model at time $t$ (see Goodfellow et al. (2016),p.372ff). 
In our example for Morsecode, by teacher forcing, the decoder LSTM network is trained to go from the given target sequences at a given timestep to the sequence one timestep in the future, i.e. the decoder-part is learning to generate targets $y_{t+1}$ given targets $y_t$, conditional on the input sequence $x_t$, as described for the English-French sequences here:   https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

## Generate Data

```{r alphabets}
latinalphabet <- c(letters, "", " ", "." , ",")

morsealphabet <- c("10111", "111010101", "11101011101", "1110101",
                   "1", "101011101", "111011101", "1010101", "101",
                   "1011101110111", "111010111", "101110101",
                   "1110111", "11101", "11101110111", "10111011101",
                   "1110111010111", "1011101", "10101", "111",
                   "1010111", "101010111", "101110111", "11101010111",
                   "1110101110111", "11101110101", "000", "0000000",
                   "10111010111010111", "1110111010101110111")

dictionary <- data.frame(cbind(latinAlphabet = latinalphabet, morseCode = morsealphabet))
```

To create a text in morsecode from a text in latin alphabet, we define the following functions: 

```{r texttomorse}
# function that translates one latin letter to morsecode (including "", " ", "." , ",")
latinToMorse <- function(l){
  morse <- paste0(dplyr::filter(dictionary, latinAlphabet == l)[, "morseCode"])
  return(morse)
}

textToMorsecode <- function(latinstr) {
  # input text should be tolower 
  latinstr <- tolower(latinstr)
  # filter for letters and symbols in latinalphabet (created for the dictionary)
  latinstr <- latinstr[which(latinstr %in% latinalphabet)]
  # apply the above function latinToMorse to latin letters+spaces
  out = sapply(latinstr, latinToMorse, USE.NAMES = FALSE)
  return(out)
}
```

The input vocabulary is $\texttt{morsealphabet}$ and the output vocabulary is $\texttt{latinalphabet}$.
As we want to predict sequences (here: words/short sentences), a simple example to start with are short sentences in English as provided at http://www.manythings.org/anki/fra-eng.zip for English-French sentence pairs. We take the English sentences and translate them to Morsecode using the function `textToMorsecode` defined above to get a set of inputs in Morsecode and targets in English.

Now, the training batch size, number of training epochs and the latent dimensionality of the training space are selected.

```{r networksettings}
# Settings 
batch_size = 64  # training batch size
epochs = 50  # number of epochs for training
latent_dim = 256  # latent dimensionality of encoding space
```

The next step is to turn the given data into target and input data that can be used for training the model. 
For this, English sequences are split, converted to all lowercase and only the elements of the target sequences are kept that also occur in the latin alphabet as defined above by `c(letters, "", " ", "." , ",")`. 
This is to ensure that special punctuation like "!" or "?" is not included.
As a last step, the target sequences are padded with `"\t"` in the front and `"\n"` in the back to indicate where the sequence starts and ends.

```{r targets}
# code partly taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

## Target in English 
# vector of sentences in English is the target output, each sentence is one entry
target_texts  <- text1[[1]] 
# split strings in single letters
target_texts  <- lapply(target_texts, function(s) strsplit(s, split="")[[1]])
# convert to lower case 
target_texts <- lapply(target_texts, function(s) tolower(s))
# only keep letters & symbols that are in latinalphabet
target_texts <- lapply(target_texts, function(s) s[which(s %in% latinalphabet)])
# pad with \t and \n for the encoder-decoder model
target_texts <- lapply(target_texts, function(s) c("\t", s , "\n"))
```

As described before, the target sequences are then translated to Morsecode to create input data using the function `textToMorsecode()` defined above.

```{r inputs}
## Input in Morsecode
# translate target text to morsecode for input text 
input_texts <- map(target_texts, textToMorsecode)
```

```{r}
# take a look at reformatted targets and inputs
target_texts[[1]]
input_texts[[1]]
```

Now, the inputs and target sequences are used to define variables that will be used by the encoder and decoder models like the maximum encoder and decoder sequence length or the number of unique input and target tokens. 

```{r}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

input_characters  <- sort(unique(unlist(input_texts)))
target_characters <- sort(unique(unlist(target_texts)))

num_encoder_tokens <- length(input_characters)
num_decoder_tokens <- length(target_characters)
# 29L

max_encoder_seq_length <- max(sapply(input_texts,length))
max_decoder_seq_length <- max(sapply(target_texts,length))
# 16L

cat('Number of samples:', length(input_texts),'\n')
cat('Number of unique input tokens:', num_encoder_tokens,'\n')
cat('Number of unique output tokens:', num_decoder_tokens,'\n')
cat('Max sequence length for inputs:', max_encoder_seq_length,'\n')
cat('Max sequence length for outputs:', max_decoder_seq_length,'\n')
```

There are two more unique characters in the output data as the target sequences are padded with `"\t"` in the front and `"\n"` in the back to indicate where the sequence starts and ends.
As a next step, the encoder and decoder input data and the decoder target data are defined by creating arrays consisting of three vectors of different sizes. 
The matrices `d1`, `d2` and `d3` are $0-1$-matrices with columns equal to either the alphabet in Morsecode or in English giving back $1$ for each column in which the unique elements of the sequences occur.
These matrices are then used to fill the second vector in the arrays of three vectors iteratively to create the final encoder and decoder input data as well as the decoder target data.

```{r define-encoder-decoder-data}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

input_token_index  <- 1:length(input_characters)
names(input_token_index) <- input_characters

target_token_index <- 1:length(target_characters)
names(target_token_index) <- target_characters

# initialize encoder, decoder input and decoder target data as 3-d arrays
encoder_input_data <- array(
  0, dim = c(length(input_texts), max_encoder_seq_length, num_encoder_tokens))
decoder_input_data <- array(
  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))
decoder_target_data <- array(
  0, dim = c(length(input_texts), max_decoder_seq_length, num_decoder_tokens))

# fill second dimensions of the 3-d arrays iteratively for all selected samples 
for(i in 1:length(input_texts)) {
  d1 <- sapply( input_characters, function(x) { as.integer(x == input_texts[[i]]) })
  encoder_input_data[i,1:nrow(d1),] <- d1
  d2 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]]) })
  decoder_input_data[i,1:nrow(d2),] <- d2
  d3 <- sapply( target_characters, function(x) { as.integer(x == target_texts[[i]][-1]) })
  decoder_target_data[i,1:nrow(d3),] <- d3
}
```

## Model definition

Now, the encoder model is built by combining an input layer and an LSTM layer. 

```{r encodermodel, message=FALSE, warning=FALSE}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

## Define an input sequence and process it.

# input layer
encoder_inputs <- layer_input(shape = list(NULL,num_encoder_tokens),
                              name = "encoder_inputs")
# LSTM-unit
encoder <- layer_lstm(units = latent_dim, 
                      return_state = TRUE,
                      name = "encoder")

# encoding network is stored in encoder_results
encoder_results <- encoder_inputs %>% encoder
# discard `encoder_outputs`
encoder_states  <- encoder_results[2:3]
```

By selecting `encoder_states <- encoder_results[2:3]`, only the states of the encoder are kept for further analyses while the outputs are discarded. 

Now, the decoder is set up and `encoder_states` are the initial states. 

```{r decodermodel}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

# set up decoder
decoder_inputs  <- layer_input(shape = list(NULL, num_decoder_tokens),
                               name = "decoder_inputs")

# set up decoder to return full output sequences & internal states as well
# return states are not used in the training model, but in inference!

decoder_lstm    <- layer_lstm(units = latent_dim, 
                              return_sequences = TRUE,
                              return_state = TRUE, 
                              stateful = FALSE,
                              name = "decoder_lstm")

decoder_results <- decoder_lstm(decoder_inputs, 
                                initial_state = encoder_states)

decoder_dense   <- layer_dense(units = num_decoder_tokens, 
                               activation = 'softmax', 
                               name = "decoder_dense")

# store decoder outputs
decoder_outputs <- decoder_dense(decoder_results[[1]])
```

To create the decoder model, an input layer is combined with an LSTM layer and a dense layer. The decoder output is then given by the output of the dense layer using the states of the LSTM layer accessible via `decoder_results[[1]]`. 
Now, one can define a model that will take the encoder input data and decoder input data and give back the decoder target data as outputs.

```{r defcomp}
## define model
model <- keras_model(inputs = list(encoder_inputs, decoder_inputs),
                     outputs = decoder_outputs)

## compile model
model %>% compile(
  optimizer='rmsprop', 
  loss='categorical_crossentropy',
  metrics = c('accuracy', 'mae'))

# history <- model %>% fit(
#   x_train, y_train,
#   batch_size = batch_size,
#   epochs = epochs,
#   verbose = 1,
#   callbacks = callback_tensorboard("logs/run_a"),
#   validation_split = 0.2
# )
```

In $\texttt{keras}$-package there are different ways to inspect the model setup defined above. One can e.g. get a summary representation of the model via: 

```{r}
summary(model)
```

Moreover, one can get a detailed model configuration using: 

```{r}
get_config(model)
```

The layer configuration can be accessed via `keras::get_layer()` where `index=2` or the name of the layer can be used to access e.g. the second layer.

```{r}
# the two variants given are equivalent

get_layer(model, index = 1)
#get_layer(model, "encoder_inputs")

get_layer(model, index = 2)
#get_layer(model, "decoder_inputs")

get_layer(model, index = 3)
#get_layer(model, "encoder")

get_layer(model, index = 4)
#get_layer(model, "decoder_lstm")

get_layer(model, index = 5)
#get_layer(model, "decoder_dense")
```

Moreover, it is possible to list the input tensors via:

```{r}
model$inputs
```

Remember that we added $\texttt{"\t"}$ and $\texttt{"\n"}$ to the target texts to denote the start and end of words. Also, there was no $\texttt{"000"}$ in the input text translated to morsecode. As such the encoder input has $29$ elements and the decoder input has $31$ unique input elements.

The ouput tensor can be accessed via:

```{r}
model$outputs
```

## Run Model 

After setting up the model, one can run the model for training. 
The packages $\texttt{keras}$ and $\texttt{tensorflow}$ allow for several different ways of visualizing the training and validation metrics like e.g. loss, accuracy and mean absolute error during training.

```{r, message=FALSE, warning=FALSE}
## Run model
# model %>% fit( 
#   list(encoder_input_data, decoder_input_data), 
#   decoder_target_data,
#   batch_size=batch_size,
#   epochs=epochs,
#   validation_split=0.2)

# use tensorboard for visualizations
tensorboard("logs/run_a")

hist <- model %>% fit( 
  list(encoder_input_data, decoder_input_data), 
  decoder_target_data,
  batch_size=batch_size,
  epochs=epochs,
  callbacks=callback_tensorboard("logs/run_a"),
  validation_split=0.2)

# ## Save Model
# save_model_hdf5(model,'s2s.h5')
# save_model_weights_hdf5(model,'s2s-wt.h5')
# ## Load Model
# model <- load_model_hdf5('s2s.h5')
# load_model_weights_hdf5(model,'s2s-wt.h5')
```

Using the training process stored in `history` and the base R `plot()`, one can also create a quick version of these plots for loss, accuracy and mean absolute error inside the document.

```{r plot, warning=FALSE, message=FALSE}
# hist$history is NULL - Why?

plot(hist) +
  theme_bw()
```

Another way for plotting these metrics in a slightly cleaner way is to directly access the elements e.g. `hist$metrics$loss` and `hist$metrics$val_loss` and to plot them together "by hand". 

```{r}
# plot for loss
# Plot the model loss of the training data
plot(hist$metrics$loss, main="Model Loss", xlab = "epoch", ylab="loss", col="blue", type="l")

# Plot the model loss of the test data
lines(hist$metrics$val_loss, col="green")

# Add legend
legend("topright", c("train","test"), col=c("blue", "green"), lty=c(1,1))
```

## Inference 

After training, one can now decode unknown input sequences via the steps described in detail in https://keras.rstudio.com/articles/examples/lstm_seq2seq.html:

1. encode unknown input sequence to state vectors 

2. start with first character, i.e. the target sequence has only 1 character at first

3. put state vectors and target sequence of 1 character through the decoder -> producing predictions for the following character 

4. sample next character using these predictions

5. append sampled character to the target sequence

6. repeat until end-of-sequence character (or limit of characters)

These steps are described in more detail now.
The encoder model for inference defined below is a `keras::keras_model`-object that combines the input layer defined in the training part above and the `encoder_states` of the training model.

Then, `decoder_states_inputs` are defined by combining two input layers using the `keras::layer_input()`-function.

The results of the decoder are defined using `decoder_lstm()` which is a LSTM layer defined with `keras::layer_lstm()` in the training part above. 
Additionally two arguments are set in `decoder_lstm()`, namely `object = decoder_inputs` and `initial_state=decoder_states_inputs`.

```{r}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

# define sampling models
encoder_model <- keras_model(encoder_inputs, encoder_states)

decoder_state_input_h <- layer_input(shape=latent_dim,
                                     name="decoder_state_input_h")

decoder_state_input_c <- layer_input(shape=latent_dim,
                                     name="decoder_state_input_c")

decoder_states_inputs <- c(decoder_state_input_h, decoder_state_input_c)

# decoder that returns full output sequences and internal states 
# decoder_lstm() was defined with layer_lstm() in the training part above
decoder_results <- decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)
decoder_states  <- decoder_results[2:3]
# decoder_dense() was defined with layer_dense() in the training part above 
decoder_outputs <- decoder_dense(decoder_results[[1]])

# keras_model
decoder_model <- keras_model(
  inputs = c(decoder_inputs, decoder_states_inputs),
  outputs = c(decoder_outputs, decoder_states))
```

To then decode a given previously unknown sequence, the next steps closely follow the example in using a function for decoding given input sequences as defined in https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

```{r decode-sequence}
# code taken from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

# reverse lookup token index for decoding sequences to sth readable
reverse_input_char_index  <- as.character(input_characters)
reverse_target_char_index <- as.character(target_characters)

# function that decodes a given input sequence
decode_sequence <- function(input_seq) {
  # Step 1: encode input as state vectors
  states_value <- predict(encoder_model, input_seq)
  
  # Step 2: generate empty target sequence of length 1
  target_seq <- array(0, dim=c(1, 1, num_decoder_tokens))
  # Step 3: populate first character of target sequence with start character
  target_seq[1, 1, target_token_index['\t']] <- 1.
  
  # sampling loop for a batch of sequences
  # (to simplify, here we assume a batch of size 1)
  stop_condition = FALSE
  decoded_sentence = ''
  maxiter = max_decoder_seq_length
  niter = 1
  while (!stop_condition && niter < maxiter) {
    
    # output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
    decoder_predict <- predict(decoder_model, c(list(target_seq), states_value))
    output_tokens <- decoder_predict[[1]]
    
    # Step 4: sample a token
    sampled_token_index <- which.max(output_tokens[1, 1, ])
    sampled_char <- reverse_target_char_index[sampled_token_index]
    decoded_sentence <-  paste0(decoded_sentence, sampled_char)
    decoded_sentence
    
    # Step 6: exit if max length or stop character is reached
    if (sampled_char == '\n' ||
        length(decoded_sentence) > max_decoder_seq_length) {
      stop_condition = TRUE
    }
    
    # Step 5:  update the target sequence (of length 1).
    target_seq[1, 1, ] <- 0
    target_seq[1, 1, sampled_token_index] <- 1.
    
    # update states
    h <- decoder_predict[[2]]
    c <- decoder_predict[[3]]
    states_value = list(h, c)
    niter <- niter + 1
  }    
  return(decoded_sentence)
}
```

Now, the following `for`-loop iterates over a sample of the input data and decodes. 

Is this really new data, though? 

```{r decode}
for (seq_index in 1:100) {
  input_seq = encoder_input_data[seq_index,,,drop=FALSE]
  decoded_sentence = decode_sequence(input_seq)
  target_sentence <- gsub("\t|\n","",paste(target_texts[[seq_index]],collapse=''))
  input_sentence  <- paste(input_texts[[seq_index]],collapse='')
  cat('-\n')
  cat('Input sentence  : ', input_sentence,'\n')
  cat('Target sentence : ', target_sentence,'\n')
  cat('Decoded sentence: ', decoded_sentence,'\n')
}
```

# WORK-IN-PROGRESS: Look "inside" a LSTM network

Ideas:

visualize LSTM activations "by training a denoising autoencoder on LSTM layer activations": 
https://medium.com/asap-report/visualizing-lstm-networks-part-i-f1d3fa6aace7

visualize LSTM gate activations: 
https://github.com/keras-team/keras/issues/1922
https://github.com/Praneet9/Visualising-LSTM-Activations/blob/master/Text_Generation.ipynb

```{r}
# get input at a specific layer
get_input_at(model, 0)
get_input_at(model, 1)

# get output at specific layer
get_output_at(model, 0)
get_output_at(model, 1)
```

```{r}
# ǵet weights of the model via ...$variables or ...$weights
vars <- model$variables
ws <- model$weights
# vars seems to be quivalent to ws
```

Another idea could be to run intermediate models to access the output of intermediate layers via:

```{r}
layer_name1 <- "encoder"

intermediate_layer_model1 <- keras_model(inputs = list(encoder_inputs, decoder_inputs),
                                        outputs = get_layer(model, layer_name1)$output)

intermediate_output1 <- predict(intermediate_layer_model1, encoder_input_data)
```

# References

https://keras.rstudio.com/articles/examples/lstm_seq2seq.html

Chollet, F. and Allaire, JJ et al. (2017): R Interface to Keras. https://github.com/rstudio/keras

Goodfellow et al. (2016): Deep Learning. Adaptive Computation and Machine Learning. The MIT Press, Massachusetts.

Hochreiter, S. and Schmidhuber, J. (1997): Long Short-term Memory. Neural Computation 9(8):1735-80. https://doi.org/10.1162/neco.1997.9.8.1735